{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "In imaging studies, PHI is generally wiped out by applying anonymization rules on DICOM tags, combined with the CRPs' efforts to delete series with PHI, or redact information burned on the images.\n",
    "\n",
    "However, in rare cases, PHI might come through in studies/series' descriptions. For example, the description would include the subjects' names.\n",
    "\n",
    "# Solution\n",
    "\n",
    "Using NLTK library, the notebook finds low frequencies words in the studies uploaded to the specified Inteleshare project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "### 1. Setup Inteleshare project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AMBRA_Utils\n",
    "import itertools\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from phi_data import non_phi, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambra_account_name = \"MOST\"\n",
    "ambra = AMBRA_Utils.utilities.get_api()\n",
    "account = ambra.get_account_by_name(ambra_account_name)\n",
    "namespace = account.get_location_by_name(\"3 - Assigned Studies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_json = '../Files/json/phi.json'\n",
    "non_phi_json = '../Files/json/non_phi.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get all studies and series' descriptions frequency dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_phi_excel(file_name: str, data: list):\n",
    "    \"\"\"\n",
    "    Create an excel which user can mark which word is PHI\n",
    "    in `data`.\n",
    "\n",
    "    Inputs:\n",
    "    --------\n",
    "    file_name (str):\n",
    "        Excel file name.\n",
    "\n",
    "    data (list):\n",
    "        List of tuples (`word`, `frequency`).\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame.from_records(data, columns=[\"word\", \"frequency\"])\n",
    "    df[\"PHI\"] = \"\"\n",
    "    df.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "studies = list(namespace.get_studies())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studies_desc_tokens = []\n",
    "series_desc_tokens = []\n",
    "series_study_map = dict()\n",
    "\n",
    "# Get all words from all studies and series' descriptions\n",
    "for study in studies:\n",
    "    study_tokens = word_tokenize(\n",
    "        \" \".join(study.formatted_description.split(\"_\")).lower()\n",
    "    )\n",
    "    studies_desc_tokens.append(study_tokens)\n",
    "\n",
    "    series = study.get_series()\n",
    "    for s in series:\n",
    "        s_desc_split = s.formatted_description.split(\"_\")\n",
    "        s_desc_split = [char.lower() for char in s_desc_split]\n",
    "        for word in s_desc_split:\n",
    "            if word not in series_study_map:\n",
    "                series_study_map[word] = []\n",
    "            series_study_map[word].append(study)\n",
    "\n",
    "        s_tokens = word_tokenize(\" \".join(s_desc_split))\n",
    "        series_desc_tokens.append(s_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mra', 47),\n",
       " ('rapid', 39),\n",
       " ('hrs', 31),\n",
       " ('72', 15),\n",
       " ('36', 15),\n",
       " ('spine', 9),\n",
       " ('c', 8),\n",
       " ('perfusion', 8),\n",
       " ('summary', 7),\n",
       " ('automated', 6),\n",
       " ('48', 5),\n",
       " ('hours', 4),\n",
       " ('viz', 4),\n",
       " ('no', 4),\n",
       " ('2', 3),\n",
       " ('time', 3),\n",
       " ('msu', 3),\n",
       " ('hemicrani', 2),\n",
       " ('braiin', 2),\n",
       " ('baseliine', 2),\n",
       " ('t', 2),\n",
       " ('event', 2),\n",
       " ('incomplete', 1),\n",
       " ('study', 1),\n",
       " ('mrp', 1),\n",
       " ('very', 1),\n",
       " ('12', 1),\n",
       " ('06pm', 1),\n",
       " ('vs', 1),\n",
       " ('am', 1),\n",
       " ('not', 1),\n",
       " ('included', 1),\n",
       " ('in', 1),\n",
       " ('dicom', 1),\n",
       " ('header', 1),\n",
       " ('correct', 1),\n",
       " ('date', 1),\n",
       " ('9', 1),\n",
       " ('25', 1),\n",
       " ('22', 1),\n",
       " ('at', 1),\n",
       " ('01', 1),\n",
       " ('45am', 1),\n",
       " ('72hrs', 1),\n",
       " ('xa', 1),\n",
       " ('reprocessed', 1),\n",
       " ('lica', 1),\n",
       " ('stenting', 1),\n",
       " ('unscheduled1', 1),\n",
       " ('l', 1),\n",
       " ('2nd', 1),\n",
       " ('enrolling', 1),\n",
       " ('basline', 1),\n",
       " ('baseine', 1),\n",
       " ('image', 1),\n",
       " ('vpct', 1),\n",
       " ('ncct', 1),\n",
       " ('processing', 1),\n",
       " ('failure', 1),\n",
       " ('read', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process studies\n",
    "\n",
    "studies_desc_tokens_flat = list(itertools.chain.from_iterable(studies_desc_tokens))\n",
    "\n",
    "\n",
    "# Remove words that are non-PHI\n",
    "studies_desc_tokens_flat = [x for x in studies_desc_tokens_flat if x not in non_phi]\n",
    "\n",
    "# Warn if PHI found\n",
    "for token in studies_desc_tokens_flat:\n",
    "    if token in phi:\n",
    "        print(f'{token} is PHI!')\n",
    "\n",
    "\n",
    "studies_desc_freq = FreqDist(studies_desc_tokens_flat)\n",
    "\n",
    "# Get least frequent\n",
    "num = 60\n",
    "studies_least_common = studies_desc_freq.most_common()[-num:]\n",
    "\n",
    "# Manually check if studies desc contains PHI\n",
    "studies_least_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_phi_excel(f\"studies_phi_{datetime.now()}.xlsx\", studies_least_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process series\n",
    "\n",
    "series_desc_tokens_flat = list(itertools.chain.from_iterable(series_desc_tokens))\n",
    "\n",
    "# Remove hash strings by ignoring strings with more than 10 characters\n",
    "# and have more than 6 numbers\n",
    "series_desc_tokens_flat_filtered = []\n",
    "for series_token in series_desc_tokens_flat:\n",
    "    num_count = 0\n",
    "    for char in series_token:\n",
    "        if char.isdigit():\n",
    "            num_count += 1\n",
    "\n",
    "    if not (num_count >= 6 and len(series_desc_tokens) >= 10):\n",
    "        series_desc_tokens_flat_filtered.append(series_token)\n",
    "\n",
    "# Remove words that are non-PHI\n",
    "series_desc_tokens_flat_filtered = [x for x in series_desc_tokens_flat_filtered if x not in non_phi]\n",
    "\n",
    "# Warn if PHI found\n",
    "for token in series_desc_tokens_flat_filtered:\n",
    "    if token in phi:\n",
    "        print(f'{token} is PHI!')\n",
    "\n",
    "series_desc_freq = FreqDist(series_desc_tokens_flat_filtered)\n",
    "\n",
    "# Get least frequent\n",
    "num = 500\n",
    "series_least_common = series_desc_freq.most_common()[-num:]\n",
    "\n",
    "# Manually check if studies desc contains PHI\n",
    "series_least_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Mark PHI or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be two new files generated, one for series description, and one for studies description. Mark PHI by following these steps:\n",
    "\n",
    "1. Open the files in Excel.\n",
    "2. Skim through each row, and for rows with PHI, mark 'x' in `PHI` column.\n",
    "- When not obvious, check the entire description of the series/studies by looking it up. \n",
    "    + Study lookup: Lookup the word on Inteleshare in the specified namespace.\n",
    "    + Series lookup: Run `series_studies_map[{word_to_lookup}]`\n",
    "3. Save the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Import marked Excel into data for PHI and non-PHI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get marked Excel files\n",
    "\n",
    "studies_phi_df = pd.read_excel('./studies_phi_2025-03-27 12:51:57.838670.xlsx')\n",
    "series_phi_df = pd.read_excel('./series_phi_2025-03-27 15:32:37.548191.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_phi_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Import data from `df` to assign into PHI and non-PHI files.\n",
    "\n",
    "    Inputs:\n",
    "    --------\n",
    "    df (pd.DataFrame): \n",
    "        A dataframe with words, its frequency and whether it's marked as PHI.\n",
    "    \"\"\"\n",
    "    with open(phi_json, 'r') as phi_f, open(non_phi_json) as non_phi_f:\n",
    "        phi_json = json.load(phi_f)\n",
    "        non_phi_json = json.load(non_phi_f)\n",
    "\n",
    "        # Add PHI and non-PHI data into json\n",
    "        for index, row in df.iterrows():\n",
    "            row_data = {\n",
    "                'word': row['word'],\n",
    "                'trial': ambra_account_name\n",
    "            }\n",
    "            if row['PHI'] == 'x':\n",
    "                phi_json['data'].append(row_data)\n",
    "                phi.add(row['word'])\n",
    "            else:\n",
    "                non_phi_json['data'].append(row_data)\n",
    "                non_phi.add(row['word'])\n",
    "\n",
    "        phi_json['trials'].append(ambra_account_name)\n",
    "        non_phi_json['trials'].append(ambra_account_name)\n",
    "\n",
    "    # Write to JSON\n",
    "    with open(phi_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(phi_json, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    with open(non_phi_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(non_phi_json, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_phi_data(series_phi_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redcap-ambra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
